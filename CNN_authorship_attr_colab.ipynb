{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import os\n",
    "#os.environ['KERAS_BACKEND']='theano' # Why theano why not\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone -l -s https://github.com/banglakit/bengali-stemmer.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/banglakit/bengali-stemmer.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bengali_stemmer.rafikamal2014 import RafiStemmer\n",
    "stemmer = RafiStemmer()\n",
    "stemmer.stem_word('করোনাভাইরাসে')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = '/content/drive/My Drive/Colab Notebooks'\n",
    "\n",
    "data_train = pd.read_csv(data_url + '/ulm_train.csv')\n",
    "data_test = pd.read_csv(data_url + '/ulm_test.csv')\n",
    "stopwords = pd.read_csv(data_url + '/Stopwords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data_train.drop(columns='is_valid')\n",
    "test = data_test.drop(columns='is_valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone -l -s https://github.com/sagorbrur/bnlp.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bnlp_toolkit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_punct(sentence):\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#|।|’|‘]', r'', sentence)\n",
    "    cleaned1 = re.sub(r'[.|,|(|)|\\|/]', r'', cleaned)\n",
    "    cleaned = re.sub(r'[০|১|২|৩|৪|৫|৬|৭|৮|৯]', r'', cleaned1)\n",
    "    cleaned1 = re.sub(r'[-|=]', r' ', cleaned)\n",
    "    return cleaned1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_stop = set(stopwords['words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(data):\n",
    "    i=0\n",
    "    str1=' '\n",
    "    final_string = []\n",
    "    final_words = []\n",
    "    all_negative_words = []\n",
    "    s=''\n",
    "\n",
    "    for sentence in data:\n",
    "        filtered_sentence = []\n",
    "\n",
    "        for w in sentence.split():\n",
    "            for cleaned_word in clean_punct(w).split():\n",
    "                if len(cleaned_word)>2:\n",
    "                    if((cleaned_word) not in set_stop):\n",
    "                        s = stemmer.stem_word(cleaned_word)\n",
    "                        if len(s)>2:\n",
    "                            final_words.append(s)\n",
    "                            filtered_sentence.append(s)\n",
    "                    else:\n",
    "                        continue\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "        str1 = \" \".join(filtered_sentence)\n",
    "        final_string.append(str1)\n",
    "    return final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "macronum=sorted(set(train['label']))\n",
    "macro_to_id = dict((note, number) for number, note in enumerate(macronum))\n",
    "\n",
    "def fun(i):\n",
    "    return macro_to_id[i]\n",
    "\n",
    "train['label']=train['label'].apply(fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macronum=sorted(set(total_data['label']))\n",
    "macro_to_id = dict((note, number) for number, note in enumerate(macronum))\n",
    "\n",
    "def fun(i):\n",
    "    return macro_to_id[i]\n",
    "\n",
    "total_data['label']=total_data['label'].apply(fun)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 128\n",
    "num_classes = 6\n",
    "epochs = 12\n",
    "\n",
    "def get_labels(data):\n",
    "  labels = []\n",
    "  for idx in data['label']:\n",
    "      labels.append(idx)\n",
    "  # labels = to_categorical(np.asarray(labels))\n",
    "  labels = to_categorical(np.asarray(labels), num_classes)\n",
    "  return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = pre_process(total_data['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bnlp.nltk_tokenizer import NLTK_Tokenizer\n",
    "# text = \"আমি ভাত খাই। সে বাজারে যায়। তিনি কি সত্যিই ভালো মানুষ?\"\n",
    "bnltk = NLTK_Tokenizer()\n",
    "# word_tokens = bnltk.word_tokenize(text)\n",
    "# sentence_tokens = bnltk.sentence_tokenize(text)\n",
    "# print(word_tokens)\n",
    "# print(sentence_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
