{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __init.py file\n",
    "from __future__ import print_function\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bow.py, here applying tfidf + Logistic Regression and some data preprocessing\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from utils import loadWord2Vec\n",
    "from math import log\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "# build corpus\n",
    "dataset = '20ng' # 20 ng is dataset which one store in data/20ng directorey\n",
    "\n",
    "# shulffing\n",
    "doc_name_list = []\n",
    "doc_train_list = []\n",
    "doc_test_list = []\n",
    "\n",
    "f = open('data/' + dataset + '.txt', 'r') # open 'data' directory, open the '20ng' dataset as a text extension,only read,here we find only doc_name\n",
    "for line in f.readlines():  # iterate through the each line \n",
    "    doc_name_list.append(line.strip()) # each line of dataset we append it onto list,strip() method returns a copy of the \n",
    "    # string by removing both leading and trailing charachters\n",
    "    temp = line.split(\"\\t\") # this line divide by each word and store it to temp list,for checking 'test' word\n",
    "    if temp[1].find('test') != -1: # if the first line store the name 'test' \n",
    "        doc_test_list.append(line.strip()) # if this line is test then apend in 'doc_test_list',strip means remove leading and trailing char \n",
    "    elif temp[1].find('train') != -1: # the first line has 'train' name(checks which are not match with -1)\n",
    "        doc_train_list.append(line.strip()) # if the line has train then store it to 'doc_train_list' \n",
    "f.close()  # close the python file\n",
    "\n",
    "doc_content_list = [] \n",
    "f = open('data/corpus/' + dataset + '.clean.txt', 'r') # open the data corpus as read mode,here we find the doc_content\n",
    "for line in f.readlines(): # each line of readlines file\n",
    "    doc_content_list.append(line.strip()) # we append without first and last word(using strip) in a list\n",
    "f.close()\n",
    "\n",
    "train_ids = []\n",
    "for train_name in doc_train_list: # 'doc_train_list' store all of the train data \n",
    "    train_id = doc_name_list.index(train_name) # index rutrns the index number of each line of 'doc_train_list'\n",
    "    train_ids.append(train_id) # append the all 'train_id' in 'train_ids' list\n",
    "print(train_ids)\n",
    "random.shuffle(train_ids) # randomly shuffle the all train_ids\n",
    "\n",
    "# partial labeled data\n",
    "\n",
    "\n",
    "f = open('data/' + dataset + '.train.index', 'r')  # take the partial labeled data\n",
    "lines = f.readlines() # take the all lines in 'lines' variable\n",
    "f.close() # close the file\n",
    "train_ids = [int(x.strip()) for x in lines] # each line of partial labeled data we remove first and last word and store it to 'train_ids' \n",
    "\n",
    "\n",
    "#train_ids = train_ids[:int(0.2 * len(train_ids))]\n",
    "\n",
    "test_ids = []\n",
    "for test_name in doc_test_list: # take the each line of 'doc_test_list'\n",
    "    test_id = doc_name_list.index(test_name) # find the index of each 'test_name' and store it to 'test_id'\n",
    "    test_ids.append(test_id)  # append the all test_id into 'test_ids' list\n",
    "print(test_ids)\n",
    "random.shuffle(test_ids) # now shuffle the test_ids list,here it doesn't preserve a order\n",
    "\n",
    "ids = train_ids + test_ids # take the all train_ids and test_ids and print it\n",
    "print(ids)\n",
    "print(len(ids))\n",
    "\n",
    "train_size = len(train_ids) # take the length of 'train_ids'\n",
    "val_size = int(0.1 * train_size) # if the 'train_size' is 32,then val_size is 3\n",
    "real_train_size = train_size - val_size # now real_train_size is 32-3 = 29\n",
    "\n",
    "shuffle_doc_name_list = []\n",
    "shuffle_doc_words_list = []\n",
    "for id in ids: # here iterate through all train_ids and test_ids(which are shuffle up previously)\n",
    "    shuffle_doc_name_list.append(doc_name_list[int(id)]) # store all doc_name where ids are previously shuffled(from 'data' directory)\n",
    "    shuffle_doc_words_list.append(doc_content_list[int(id)]) # store all doc_content where ids are previously shuffled (from 'data/corpus' directory)\n",
    "\n",
    "tfidf_vec = TfidfVectorizer() #max_features=50000, here 'tfidf_vec' variable defines the TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vec.fit_transform(shuffle_doc_words_list) # applying tfidf in 'shuffle_doc_words_list(which contains doc_content)'\n",
    "print(tfidf_matrix)\n",
    "#tfidf_matrix_array = tfidf_matrix.toarray()\n",
    "\n",
    "# BOW TFIDF + LR\n",
    "\n",
    "#train_x = []\n",
    "train_y = []\n",
    "\n",
    "#test_x = []\n",
    "test_y = []\n",
    "\n",
    "for i in range(len(shuffle_doc_words_list)): # iterate untill len of shuffled 'doc_content'\n",
    "    doc_words = shuffle_doc_words_list[i] # store each 'shuffle_doc_words' as a 'doc_words'\n",
    "    words = doc_words.split(' ') # here each doc words split single words with a space \n",
    "\n",
    "    doc_meta = shuffle_doc_name_list[i] # store each 'shuffle_doc_name' as a 'doc_meta'\n",
    "    temp = doc_meta.split('\\t')  # split the doc_meta with a tab and store it to 'temp' variable\n",
    "    label = temp[2] # 2nd index contains the label\n",
    "\n",
    "    if i < train_size: # when the length is smaller then train_size then the label of train store it to 'train_y' list\n",
    "        #train_x.append(tfidf_matrix_array[i])\n",
    "        train_y.append(label)\n",
    "    else:   # if the length is greater then train_size which means it is test data then the label append to 'test_y' lsit\n",
    "        #test_x.append(tfidf_matrix_array[i])\n",
    "        test_y.append(label)\n",
    "\n",
    "#clf = svm.SVC(decision_function_shape='ovr', class_weight=\"balanced\",kernel='linear') # if we use svm classifier then use it\n",
    "#clf = LinearSVC(random_state=0) \n",
    "clf = LogisticRegression(random_state=1) # here declare LogisticRegression as a 'clf',and you have to define the random_state\n",
    "\n",
    "clf.fit(tfidf_matrix[:train_size], train_y) # applying Log Reg untill the length of train_size,here first argument is train data with tfidf using train size,2nd argument is train data label(o/p) \n",
    "predict_y = clf.predict(tfidf_matrix[train_size:]) # here predict the o/p using test_data(where applying tfidf,using test_data size), [:train_size] means start with 0th index until 'train_data' size and [train_data:] means after the size of 'train_data' untill last means test data\n",
    "\n",
    "correct_count = 0\n",
    "for i in range(len(test_y)): # here count the predicted test o/p match with original test o/p(label) and count it\n",
    "    if predict_y[i] == test_y[i]: # if the original o/p(label) is same as test o/p then increase the count \n",
    "        correct_count += 1\n",
    "\n",
    "accuracy = correct_count * 1.0 / len(test_y) # find the test accuracy,you can omit 1,divide by len of test_data,if you try to find the per. then mult with 100\n",
    "print(dataset, accuracy)\n",
    "\n",
    "print(\"Precision, Recall and F1-Score...\")\n",
    "print(metrics.classification_report(test_y, predict_y, digits=4)) # here predict the Precisin, Recall and F1-Score,the parameter is test o/p(label),predicted o/p and takes 4 number after decimal points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# build corpus, Read the data corpus then write this corpus in 'data/corpus' directory as a 'corpus_str.txt' file\n",
    "\n",
    "dataset = '20ng' # using 20ng dataset\n",
    "\n",
    "f = open('data/' + dataset + '.txt', 'r') # '20ng' dataset where store into 'data' directory on read mode,read the 'data' directory which file has '.txt' extension\n",
    "lines = f.readlines() # the readlines() method returns a list containing each line in a file as a list item\n",
    "docs = []\n",
    "for line in lines: # iterate each line in on all of lines where each line as a list item\n",
    "    temp = line.split(\"\\t\") # each line make a list of words with space and store it to 'temp' variable,make a list of words with tab using split\n",
    "    doc_file = open(temp[0], 'r') # take the 0th index word of 'temp' variable in 'doc_file'\n",
    "    doc_content = doc_file.read() # read the doc_file and store it to doc_content variable\n",
    "    doc_file.close() # close the file\n",
    "    print(temp[0], doc_content) # print the lines of first word,and doc_content which store the read mode of doc_file\n",
    "    doc_content = doc_content.replace('\\n', ' ') # replace new line with a space in a 'doc_content'\n",
    "    docs.append(doc_content) # each 'doc_content' append in docs list\n",
    "\n",
    "\n",
    "corpus_str = '\\n'.join(docs) # after each docs here add a 'new line' automatically\n",
    "f.close()\n",
    "\n",
    "f = open('data/corpus/' + dataset + '.txt', 'w') # open a file from 'data/corpus' using write purpose\n",
    "f.write(corpus_str) # write this 'corpus_str' into 'data/corpus' directory\n",
    "f.close()\n",
    "\n",
    "\n",
    "'''\n",
    "# datasets from PTE paper\n",
    "f = open('data/dblp/label_train.txt', 'r')\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "doc_id = 0\n",
    "doc_name_list = []\n",
    "for line in lines:\n",
    "    string = str(doc_id) + '\\t' + 'train' + '\\t' + line.strip()\n",
    "    doc_name_list.append(string)\n",
    "    doc_id += 1\n",
    "\n",
    "f = open('data/dblp/label_test.txt', 'r')\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "for line in lines:\n",
    "    string = str(doc_id) + '\\t' + 'test' + '\\t' + line.strip()\n",
    "    doc_name_list.append(string)\n",
    "    doc_id += 1\n",
    "\n",
    "doc_list_str = '\\n'.join(doc_name_list)\n",
    "\n",
    "f = open('data/dblp.txt', 'w')\n",
    "f.write(doc_list_str)\n",
    "f.close()\n",
    "\n",
    "# TREC, R8, R52, WebKB\n",
    "\n",
    "dataset = 'R52'\n",
    "\n",
    "f = open('data/' + dataset + '/train.txt', 'r')\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "doc_id = 0\n",
    "doc_name_list = []\n",
    "doc_content_list = []\n",
    "\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    label = line[:line.find('\\t')]\n",
    "    content = line[line.find('\\t') + 1:]\n",
    "    string = str(doc_id) + '\\t' + 'train' + '\\t' + label\n",
    "    doc_name_list.append(string)\n",
    "    doc_content_list.append(content)\n",
    "    doc_id += 1\n",
    "\n",
    "f = open('data/' + dataset + '/test.txt', 'r')\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    label = line[:line.find('\\t')]\n",
    "    content = line[line.find('\\t') + 1:]\n",
    "    string = str(doc_id) + '\\t' + 'test' + '\\t' + label\n",
    "    doc_name_list.append(string)\n",
    "    doc_content_list.append(content)\n",
    "    doc_id += 1\n",
    "\n",
    "doc_list_str = '\\n'.join(doc_name_list)\n",
    "\n",
    "f = open('data/' + dataset + '.txt', 'w')\n",
    "f.write(doc_list_str)\n",
    "f.close()\n",
    "\n",
    "doc_name_list_str = '\\n'.join(doc_name_list)\n",
    "\n",
    "f = open('data/' + dataset + '.txt', 'w')\n",
    "f.write(doc_list_str)\n",
    "f.close()\n",
    "\n",
    "doc_content_list_str = '\\n'.join(doc_content_list)\n",
    "\n",
    "f = open('data/corpus/' + dataset + '.txt', 'w')\n",
    "f.write(doc_content_list_str)\n",
    "f.close()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build graph\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from utils import loadWord2Vec, clean_str\n",
    "from math import log\n",
    "from sklearn import svm\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sys\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# sys.argv() is an array for command line arguments in python,len() function is used to count the number of arguments passed to \n",
    "# the command line \n",
    "if len(sys.argv) != 2: \n",
    "\tsys.exit(\"Use: python build_graph.py <dataset>\")\n",
    "\n",
    "datasets = ['20ng', 'R8', 'R52', 'ohsumed', 'mr'] # all datasets is in the 'datasets' list\n",
    "\n",
    "dataset = sys.argv[1] # build corpus \n",
    " \n",
    "if dataset not in datasets: \n",
    "\tsys.exit(\"wrong dataset name\")\n",
    "\n",
    "# Read Word Vectors\n",
    "# word_vector_file = 'data/glove.6B/glove.6B.300d.txt'\n",
    "# word_vector_file = 'data/corpus/' + dataset + '_word_vectors.txt'\n",
    "#_, embd, word_vector_map = loadWord2Vec(word_vector_file)\n",
    "# word_embeddings_dim = len(embd[0])\n",
    "\n",
    "word_embeddings_dim = 300 # declare the dim of word_embeddings \n",
    "word_vector_map = {} # started from here()\n",
    "\n",
    "# shulffing\n",
    "doc_name_list = []\n",
    "doc_train_list = []\n",
    "doc_test_list = []\n",
    "\n",
    "f = open('data/' + dataset + '.txt', 'r')\n",
    "lines = f.readlines()\n",
    "for line in lines:\n",
    "    doc_name_list.append(line.strip())\n",
    "    temp = line.split(\"\\t\")\n",
    "    if temp[1].find('test') != -1:\n",
    "        doc_test_list.append(line.strip())\n",
    "    elif temp[1].find('train') != -1:\n",
    "        doc_train_list.append(line.strip())\n",
    "f.close()\n",
    "# print(doc_train_list)\n",
    "# print(doc_test_list)\n",
    "\n",
    "doc_content_list = []\n",
    "f = open('data/corpus/' + dataset + '.clean.txt', 'r')\n",
    "lines = f.readlines()\n",
    "for line in lines:\n",
    "    doc_content_list.append(line.strip())\n",
    "f.close()\n",
    "# print(doc_content_list)\n",
    "\n",
    "train_ids = []\n",
    "for train_name in doc_train_list:\n",
    "    train_id = doc_name_list.index(train_name)\n",
    "    train_ids.append(train_id)\n",
    "print(train_ids)\n",
    "random.shuffle(train_ids)\n",
    "\n",
    "# partial labeled data\n",
    "#train_ids = train_ids[:int(0.2 * len(train_ids))]\n",
    "\n",
    "train_ids_str = '\\n'.join(str(index) for index in train_ids)\n",
    "f = open('data/' + dataset + '.train.index', 'w')\n",
    "f.write(train_ids_str)\n",
    "f.close()\n",
    "\n",
    "test_ids = []\n",
    "for test_name in doc_test_list:\n",
    "    test_id = doc_name_list.index(test_name)\n",
    "    test_ids.append(test_id)\n",
    "print(test_ids)\n",
    "random.shuffle(test_ids)\n",
    "\n",
    "test_ids_str = '\\n'.join(str(index) for index in test_ids)\n",
    "f = open('data/' + dataset + '.test.index', 'w')\n",
    "f.write(test_ids_str)\n",
    "f.close()\n",
    "\n",
    "ids = train_ids + test_ids\n",
    "print(ids)\n",
    "print(len(ids))\n",
    "\n",
    "shuffle_doc_name_list = []\n",
    "shuffle_doc_words_list = []\n",
    "for id in ids:\n",
    "    shuffle_doc_name_list.append(doc_name_list[int(id)])\n",
    "    shuffle_doc_words_list.append(doc_content_list[int(id)])\n",
    "shuffle_doc_name_str = '\\n'.join(shuffle_doc_name_list)\n",
    "shuffle_doc_words_str = '\\n'.join(shuffle_doc_words_list)\n",
    "\n",
    "f = open('data/' + dataset + '_shuffle.txt', 'w')\n",
    "f.write(shuffle_doc_name_str)\n",
    "f.close()\n",
    "\n",
    "f = open('data/corpus/' + dataset + '_shuffle.txt', 'w')\n",
    "f.write(shuffle_doc_words_str)\n",
    "f.close()\n",
    "\n",
    "# build vocab\n",
    "word_freq = {}\n",
    "word_set = set()\n",
    "for doc_words in shuffle_doc_words_list:\n",
    "    words = doc_words.split()\n",
    "    for word in words:\n",
    "        word_set.add(word)\n",
    "        if word in word_freq:\n",
    "            word_freq[word] += 1\n",
    "        else:\n",
    "            word_freq[word] = 1\n",
    "\n",
    "vocab = list(word_set)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "word_doc_list = {}\n",
    "\n",
    "for i in range(len(shuffle_doc_words_list)):\n",
    "    doc_words = shuffle_doc_words_list[i]\n",
    "    words = doc_words.split()\n",
    "    appeared = set()\n",
    "    for word in words:\n",
    "        if word in appeared:\n",
    "            continue\n",
    "        if word in word_doc_list:\n",
    "            doc_list = word_doc_list[word]\n",
    "            doc_list.append(i)\n",
    "            word_doc_list[word] = doc_list\n",
    "        else:\n",
    "            word_doc_list[word] = [i]\n",
    "        appeared.add(word)\n",
    "\n",
    "word_doc_freq = {}\n",
    "for word, doc_list in word_doc_list.items():\n",
    "    word_doc_freq[word] = len(doc_list)\n",
    "\n",
    "word_id_map = {}\n",
    "for i in range(vocab_size):\n",
    "    word_id_map[vocab[i]] = i\n",
    "\n",
    "vocab_str = '\\n'.join(vocab)\n",
    "\n",
    "f = open('data/corpus/' + dataset + '_vocab.txt', 'w')\n",
    "f.write(vocab_str)\n",
    "f.close()\n",
    "\n",
    "'''\n",
    "Word definitions begin\n",
    "'''\n",
    "'''\n",
    "definitions = []\n",
    "\n",
    "for word in vocab:\n",
    "    word = word.strip()\n",
    "    synsets = wn.synsets(clean_str(word))\n",
    "    word_defs = []\n",
    "    for synset in synsets:\n",
    "        syn_def = synset.definition()\n",
    "        word_defs.append(syn_def)\n",
    "    word_des = ' '.join(word_defs)\n",
    "    if word_des == '':\n",
    "        word_des = '<PAD>'\n",
    "    definitions.append(word_des)\n",
    "\n",
    "string = '\\n'.join(definitions)\n",
    "\n",
    "\n",
    "f = open('data/corpus/' + dataset + '_vocab_def.txt', 'w')\n",
    "f.write(string)\n",
    "f.close()\n",
    "\n",
    "tfidf_vec = TfidfVectorizer(max_features=1000)\n",
    "tfidf_matrix = tfidf_vec.fit_transform(definitions)\n",
    "tfidf_matrix_array = tfidf_matrix.toarray()\n",
    "print(tfidf_matrix_array[0], len(tfidf_matrix_array[0]))\n",
    "\n",
    "word_vectors = []\n",
    "\n",
    "for i in range(len(vocab)):\n",
    "    word = vocab[i]\n",
    "    vector = tfidf_matrix_array[i]\n",
    "    str_vector = []\n",
    "    for j in range(len(vector)):\n",
    "        str_vector.append(str(vector[j]))\n",
    "    temp = ' '.join(str_vector)\n",
    "    word_vector = word + ' ' + temp\n",
    "    word_vectors.append(word_vector)\n",
    "\n",
    "string = '\\n'.join(word_vectors)\n",
    "\n",
    "f = open('data/corpus/' + dataset + '_word_vectors.txt', 'w')\n",
    "f.write(string)\n",
    "f.close()\n",
    "\n",
    "word_vector_file = 'data/corpus/' + dataset + '_word_vectors.txt'\n",
    "_, embd, word_vector_map = loadWord2Vec(word_vector_file)\n",
    "word_embeddings_dim = len(embd[0])\n",
    "'''\n",
    "\n",
    "'''\n",
    "Word definitions end\n",
    "'''\n",
    "\n",
    "# label list\n",
    "label_set = set()\n",
    "for doc_meta in shuffle_doc_name_list:\n",
    "    temp = doc_meta.split('\\t')\n",
    "    label_set.add(temp[2])\n",
    "label_list = list(label_set)\n",
    "\n",
    "label_list_str = '\\n'.join(label_list)\n",
    "f = open('data/corpus/' + dataset + '_labels.txt', 'w')\n",
    "f.write(label_list_str)\n",
    "f.close()\n",
    "\n",
    "# x: feature vectors of training docs, no initial features\n",
    "# slect 90% training set\n",
    "train_size = len(train_ids)\n",
    "val_size = int(0.1 * train_size)\n",
    "real_train_size = train_size - val_size  # - int(0.5 * train_size)\n",
    "# different training rates\n",
    "\n",
    "real_train_doc_names = shuffle_doc_name_list[:real_train_size]\n",
    "real_train_doc_names_str = '\\n'.join(real_train_doc_names)\n",
    "\n",
    "f = open('data/' + dataset + '.real_train.name', 'w')\n",
    "f.write(real_train_doc_names_str)\n",
    "f.close()\n",
    "\n",
    "row_x = []\n",
    "col_x = []\n",
    "data_x = []\n",
    "for i in range(real_train_size):\n",
    "    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n",
    "    doc_words = shuffle_doc_words_list[i]\n",
    "    words = doc_words.split()\n",
    "    doc_len = len(words)\n",
    "    for word in words:\n",
    "        if word in word_vector_map:\n",
    "            word_vector = word_vector_map[word]\n",
    "            # print(doc_vec)\n",
    "            # print(np.array(word_vector))\n",
    "            doc_vec = doc_vec + np.array(word_vector)\n",
    "\n",
    "    for j in range(word_embeddings_dim):\n",
    "        row_x.append(i)\n",
    "        col_x.append(j)\n",
    "        # np.random.uniform(-0.25, 0.25)\n",
    "        data_x.append(doc_vec[j] / doc_len)  # doc_vec[j]/ doc_len\n",
    "\n",
    "# x = sp.csr_matrix((real_train_size, word_embeddings_dim), dtype=np.float32)\n",
    "x = sp.csr_matrix((data_x, (row_x, col_x)), shape=(\n",
    "    real_train_size, word_embeddings_dim))\n",
    "\n",
    "y = []\n",
    "for i in range(real_train_size):\n",
    "    doc_meta = shuffle_doc_name_list[i]\n",
    "    temp = doc_meta.split('\\t')\n",
    "    label = temp[2]\n",
    "    one_hot = [0 for l in range(len(label_list))]\n",
    "    label_index = label_list.index(label)\n",
    "    one_hot[label_index] = 1\n",
    "    y.append(one_hot)\n",
    "y = np.array(y)\n",
    "print(y)\n",
    "\n",
    "# tx: feature vectors of test docs, no initial features\n",
    "test_size = len(test_ids)\n",
    "\n",
    "row_tx = []\n",
    "col_tx = []\n",
    "data_tx = []\n",
    "for i in range(test_size):\n",
    "    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n",
    "    doc_words = shuffle_doc_words_list[i + train_size]\n",
    "    words = doc_words.split()\n",
    "    doc_len = len(words)\n",
    "    for word in words:\n",
    "        if word in word_vector_map:\n",
    "            word_vector = word_vector_map[word]\n",
    "            doc_vec = doc_vec + np.array(word_vector)\n",
    "\n",
    "    for j in range(word_embeddings_dim):\n",
    "        row_tx.append(i)\n",
    "        col_tx.append(j)\n",
    "        # np.random.uniform(-0.25, 0.25)\n",
    "        data_tx.append(doc_vec[j] / doc_len)  # doc_vec[j] / doc_len\n",
    "\n",
    "# tx = sp.csr_matrix((test_size, word_embeddings_dim), dtype=np.float32)\n",
    "tx = sp.csr_matrix((data_tx, (row_tx, col_tx)),\n",
    "                   shape=(test_size, word_embeddings_dim))\n",
    "\n",
    "ty = []\n",
    "for i in range(test_size):\n",
    "    doc_meta = shuffle_doc_name_list[i + train_size]\n",
    "    temp = doc_meta.split('\\t')\n",
    "    label = temp[2]\n",
    "    one_hot = [0 for l in range(len(label_list))]\n",
    "    label_index = label_list.index(label)\n",
    "    one_hot[label_index] = 1\n",
    "    ty.append(one_hot)\n",
    "ty = np.array(ty)\n",
    "print(ty)\n",
    "\n",
    "# allx: the the feature vectors of both labeled and unlabeled training instances\n",
    "# (a superset of x)\n",
    "# unlabeled training instances -> words\n",
    "\n",
    "word_vectors = np.random.uniform(-0.01, 0.01,\n",
    "                                 (vocab_size, word_embeddings_dim))\n",
    "\n",
    "for i in range(len(vocab)):\n",
    "    word = vocab[i]\n",
    "    if word in word_vector_map:\n",
    "        vector = word_vector_map[word]\n",
    "        word_vectors[i] = vector\n",
    "\n",
    "row_allx = []\n",
    "col_allx = []\n",
    "data_allx = []\n",
    "\n",
    "for i in range(train_size):\n",
    "    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n",
    "    doc_words = shuffle_doc_words_list[i]\n",
    "    words = doc_words.split()\n",
    "    doc_len = len(words)\n",
    "    for word in words:\n",
    "        if word in word_vector_map:\n",
    "            word_vector = word_vector_map[word]\n",
    "            doc_vec = doc_vec + np.array(word_vector)\n",
    "\n",
    "    for j in range(word_embeddings_dim):\n",
    "        row_allx.append(int(i))\n",
    "        col_allx.append(j)\n",
    "        # np.random.uniform(-0.25, 0.25)\n",
    "        data_allx.append(doc_vec[j] / doc_len)  # doc_vec[j]/doc_len\n",
    "for i in range(vocab_size):\n",
    "    for j in range(word_embeddings_dim):\n",
    "        row_allx.append(int(i + train_size))\n",
    "        col_allx.append(j)\n",
    "        data_allx.append(word_vectors.item((i, j)))\n",
    "\n",
    "\n",
    "row_allx = np.array(row_allx)\n",
    "col_allx = np.array(col_allx)\n",
    "data_allx = np.array(data_allx)\n",
    "\n",
    "allx = sp.csr_matrix(\n",
    "    (data_allx, (row_allx, col_allx)), shape=(train_size + vocab_size, word_embeddings_dim))\n",
    "\n",
    "ally = []\n",
    "for i in range(train_size):\n",
    "    doc_meta = shuffle_doc_name_list[i]\n",
    "    temp = doc_meta.split('\\t')\n",
    "    label = temp[2]\n",
    "    one_hot = [0 for l in range(len(label_list))]\n",
    "    label_index = label_list.index(label)\n",
    "    one_hot[label_index] = 1\n",
    "    ally.append(one_hot)\n",
    "\n",
    "for i in range(vocab_size):\n",
    "    one_hot = [0 for l in range(len(label_list))]\n",
    "    ally.append(one_hot)\n",
    "\n",
    "ally = np.array(ally)\n",
    "\n",
    "print(x.shape, y.shape, tx.shape, ty.shape, allx.shape, ally.shape)\n",
    "\n",
    "'''\n",
    "Doc word heterogeneous graph\n",
    "'''\n",
    "\n",
    "# word co-occurence with context windows\n",
    "window_size = 20\n",
    "windows = []\n",
    "\n",
    "for doc_words in shuffle_doc_words_list:\n",
    "    words = doc_words.split()\n",
    "    length = len(words)\n",
    "    if length <= window_size:\n",
    "        windows.append(words)\n",
    "    else:\n",
    "        # print(length, length - window_size + 1)\n",
    "        for j in range(length - window_size + 1):\n",
    "            window = words[j: j + window_size]\n",
    "            windows.append(window)\n",
    "            # print(window)\n",
    "\n",
    "\n",
    "word_window_freq = {}\n",
    "for window in windows:\n",
    "    appeared = set()\n",
    "    for i in range(len(window)):\n",
    "        if window[i] in appeared:\n",
    "            continue\n",
    "        if window[i] in word_window_freq:\n",
    "            word_window_freq[window[i]] += 1\n",
    "        else:\n",
    "            word_window_freq[window[i]] = 1\n",
    "        appeared.add(window[i])\n",
    "\n",
    "word_pair_count = {}\n",
    "for window in windows:\n",
    "    for i in range(1, len(window)):\n",
    "        for j in range(0, i):\n",
    "            word_i = window[i]\n",
    "            word_i_id = word_id_map[word_i]\n",
    "            word_j = window[j]\n",
    "            word_j_id = word_id_map[word_j]\n",
    "            if word_i_id == word_j_id:\n",
    "                continue\n",
    "            word_pair_str = str(word_i_id) + ',' + str(word_j_id)\n",
    "            if word_pair_str in word_pair_count:\n",
    "                word_pair_count[word_pair_str] += 1\n",
    "            else:\n",
    "                word_pair_count[word_pair_str] = 1\n",
    "            # two orders\n",
    "            word_pair_str = str(word_j_id) + ',' + str(word_i_id)\n",
    "            if word_pair_str in word_pair_count:\n",
    "                word_pair_count[word_pair_str] += 1\n",
    "            else:\n",
    "                word_pair_count[word_pair_str] = 1\n",
    "\n",
    "row = []\n",
    "col = []\n",
    "weight = []\n",
    "\n",
    "# pmi as weights\n",
    "\n",
    "num_window = len(windows)\n",
    "\n",
    "for key in word_pair_count:\n",
    "    temp = key.split(',')\n",
    "    i = int(temp[0])\n",
    "    j = int(temp[1])\n",
    "    count = word_pair_count[key]\n",
    "    word_freq_i = word_window_freq[vocab[i]]\n",
    "    word_freq_j = word_window_freq[vocab[j]]\n",
    "    pmi = log((1.0 * count / num_window) /\n",
    "              (1.0 * word_freq_i * word_freq_j/(num_window * num_window)))\n",
    "    if pmi <= 0:\n",
    "        continue\n",
    "    row.append(train_size + i)\n",
    "    col.append(train_size + j)\n",
    "    weight.append(pmi)\n",
    "\n",
    "# word vector cosine similarity as weights\n",
    "\n",
    "'''\n",
    "for i in range(vocab_size):\n",
    "    for j in range(vocab_size):\n",
    "        if vocab[i] in word_vector_map and vocab[j] in word_vector_map:\n",
    "            vector_i = np.array(word_vector_map[vocab[i]])\n",
    "            vector_j = np.array(word_vector_map[vocab[j]])\n",
    "            similarity = 1.0 - cosine(vector_i, vector_j)\n",
    "            if similarity > 0.9:\n",
    "                print(vocab[i], vocab[j], similarity)\n",
    "                row.append(train_size + i)\n",
    "                col.append(train_size + j)\n",
    "                weight.append(similarity)\n",
    "'''\n",
    "# doc word frequency\n",
    "doc_word_freq = {}\n",
    "\n",
    "for doc_id in range(len(shuffle_doc_words_list)):\n",
    "    doc_words = shuffle_doc_words_list[doc_id]\n",
    "    words = doc_words.split()\n",
    "    for word in words:\n",
    "        word_id = word_id_map[word]\n",
    "        doc_word_str = str(doc_id) + ',' + str(word_id)\n",
    "        if doc_word_str in doc_word_freq:\n",
    "            doc_word_freq[doc_word_str] += 1\n",
    "        else:\n",
    "            doc_word_freq[doc_word_str] = 1\n",
    "\n",
    "for i in range(len(shuffle_doc_words_list)):\n",
    "    doc_words = shuffle_doc_words_list[i]\n",
    "    words = doc_words.split()\n",
    "    doc_word_set = set()\n",
    "    for word in words:\n",
    "        if word in doc_word_set:\n",
    "            continue\n",
    "        j = word_id_map[word]\n",
    "        key = str(i) + ',' + str(j)\n",
    "        freq = doc_word_freq[key]\n",
    "        if i < train_size:\n",
    "            row.append(i)\n",
    "        else:\n",
    "            row.append(i + vocab_size)\n",
    "        col.append(train_size + j)\n",
    "        idf = log(1.0 * len(shuffle_doc_words_list) /\n",
    "                  word_doc_freq[vocab[j]])\n",
    "        weight.append(freq * idf)\n",
    "        doc_word_set.add(word)\n",
    "\n",
    "node_size = train_size + vocab_size + test_size\n",
    "adj = sp.csr_matrix(\n",
    "    (weight, (row, col)), shape=(node_size, node_size))\n",
    "\n",
    "# dump objects\n",
    "f = open(\"data/ind.{}.x\".format(dataset), 'wb')\n",
    "pkl.dump(x, f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"data/ind.{}.y\".format(dataset), 'wb')\n",
    "pkl.dump(y, f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"data/ind.{}.tx\".format(dataset), 'wb')\n",
    "pkl.dump(tx, f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"data/ind.{}.ty\".format(dataset), 'wb')\n",
    "pkl.dump(ty, f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"data/ind.{}.allx\".format(dataset), 'wb')\n",
    "pkl.dump(allx, f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"data/ind.{}.ally\".format(dataset), 'wb')\n",
    "pkl.dump(ally, f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"data/ind.{}.adj\".format(dataset), 'wb')\n",
    "pkl.dump(adj, f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
